<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Feng Yunji&#39;s Blog</title>
  
  
  <link href="https://blog.fengyunji.site/atom.xml" rel="self"/>
  
  <link href="https://blog.fengyunji.site/"/>
  <updated>2025-08-02T08:33:48.470Z</updated>
  <id>https://blog.fengyunji.site/</id>
  
  <author>
    <name>Feng Yunji</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NAS+tailscale点对点穿透</title>
    <link href="https://blog.fengyunji.site/NAS-tailscale%E7%82%B9%E5%AF%B9%E7%82%B9%E7%A9%BF%E9%80%8F/"/>
    <id>https://blog.fengyunji.site/NAS-tailscale%E7%82%B9%E5%AF%B9%E7%82%B9%E7%A9%BF%E9%80%8F/</id>
    <published>2025-08-02T08:18:51.000Z</published>
    <updated>2025-08-02T08:33:48.470Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NAS-Tailscale点对点穿透"><a href="#NAS-Tailscale点对点穿透" class="headerlink" title="NAS+Tailscale点对点穿透"></a>NAS+Tailscale点对点穿透</h2><p>极空间等NAS服务运营商提供官方的内网穿透服务，但在某些时候网速过慢，且有概率连接失败。因此，通过Tailscale实现点对点穿透是一个不错的选择。</p><p>与Zerotier类似，Tailscale也是基于WireGuard的点对点VPN服务。它可以在不同的设备之间建立安全的连接，而无需配置复杂的网络设置。</p><h2 id="Tailscale安装"><a href="#Tailscale安装" class="headerlink" title="Tailscale安装"></a>Tailscale安装</h2><p>Tailscale支持多种平台，包括Linux、macOS、Windows、iOS和Android。在极空间中，通过官方的docker应用下载并安装Tailscale，可以通过dockerproxy进行加速下载。</p><p>在启动Tailscale前，需要在官网中注册，并获取一个authkey，并记住这个key，只会出现一次，注册的时候需要选中这两个选项：</p><img src="2.png" alt="Tailscale Auth Key Options" width="600"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><img src="1.png" alt="Tailscale Auth Key" width="600"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><p>在docker启动时，设置以下路径映射：</p><ul><li><code>/path/to/tailscale/var/lib</code> -&gt; <code>/var/lib</code></li><li><code>/path/to/tailscale/dev/net/tun</code> -&gt; <code>/dev/net/tun</code></li></ul><p>网络设置为<code>host</code>模式</p><p>并设置以下环境变量：</p><ul><li><code>TS_AUTHKEY</code> -&gt; <code>your_auth_key</code></li><li><code>TS_ROUTES</code> -&gt; 局域网段，如: <code>192.168.1.0/24</code></li><li><code>TS_STATE_DIR</code> -&gt; <code>/var/lib/tailscale</code> # 用于设置固定ip</li></ul><p>运行Docker，此时可以在tailscale的Web控制台中看到新设备已连接，点击Edit route settings,勾选对应的路由，点击Save，最后点击Disable key expiry。</p><p>至此，完成内网穿透的配置。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NAS-Tailscale点对点穿透&quot;&gt;&lt;a href=&quot;#NAS-Tailscale点对点穿透&quot; class=&quot;headerlink&quot; title=&quot;NAS+Tailscale点对点穿透&quot;&gt;&lt;/a&gt;NAS+Tailscale点对点穿透&lt;/h2&gt;&lt;p&gt;极空间等NA</summary>
      
    
    
    
    <category term="内网穿透" scheme="https://blog.fengyunji.site/categories/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
    
    <category term="Linux" scheme="https://blog.fengyunji.site/tags/Linux/"/>
    
    <category term="NAS" scheme="https://blog.fengyunji.site/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>MASt3R-SLAM核心流程解析</title>
    <link href="https://blog.fengyunji.site/MASt3R-SLAM%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>https://blog.fengyunji.site/MASt3R-SLAM%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</id>
    <published>2025-07-21T12:20:26.000Z</published>
    <updated>2025-07-21T13:46:23.632Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前端跟踪"><a href="#前端跟踪" class="headerlink" title="前端跟踪"></a>前端跟踪</h1><p>MASt3R-SLAM的核心在于利用MASt3R模型进行稠密的多视图立体匹配，而不是传统SLAM中的稀疏特征点匹配。MASt3R能为图像中的每个像素提供：</p><ul><li>3D点云(X)</li><li>置信度(C)</li><li>密集描述子(D)</li><li>描述子置信度(Q)</li></ul><h2 id="对称推理"><a href="#对称推理" class="headerlink" title="对称推理"></a>对称推理</h2><p>在初始只有一帧图像的时候，通过复制输入图像，可以得到初始的点云，此时等价于使用MASt3R模型实现单目深度估计。随着新图像的加入，MASt3R-SLAM会在每个新图像上进行对称推理，生成新的点云和描述子。</p><h2 id="两帧点云匹配"><a href="#两帧点云匹配" class="headerlink" title="两帧点云匹配"></a>两帧点云匹配</h2><p><strong>核心：match算法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">idx_i2j, valid_match_j = matching.<span class="keyword">match</span>(</span><br><span class="line">    Xii, Xji, Dii, Dji, idx_1_to_2_init=idx_i2j_init</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><code>Xii</code>和<code>Xji</code>分别是当前帧和上一帧的点云，<code>Xji</code>是上一帧点云在当前帧坐标系下的结果；</li><li><code>Dii</code>和<code>Dji</code>分别是当前帧和上一帧的密集描述子，&#96;Dji是当前帧视角下的表示；</li></ul><p>在match函数中，首先调用<code>prep_for_iter_proj</code>函数将每个像素的3D点归一化为从相机出发的单位射线，并根据Scharr算子计算像素梯度。<br>归一化使用<code>torch.nn.functional.normalize</code>函数，确保每个像素的3D点在单位球面上，同时，初始化p作为匹配的索引，<code>p[b, i, :]</code>表示第<code>b</code>个batch中第<code>i</code>个目标3D点的初始投影像素坐标为<code>[u, v]</code>，代表了对于目标帧（上一帧）中的每个3D点，猜测在源帧中应该投影到哪个像素位置。</p><p><code>p</code>的计算使用cuda实现，在<code>iter_proj</code>函数中，输入为：</p><ul><li><code>rays_with_grad_img</code>，源图像的射线+梯度信息</li><li><code>pts3d_norm</code>，目标图像的3D点归一化</li><li><code>p</code>，初始投影位置<br>核心是Levenberg-Marquardt迭代优化，使用CUDA并行策略，误差的计算为当前射线与目标射线的欧氏距离平方：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;<span class="number">3</span>; j++) &#123;</span><br><span class="line">    err[j] = r[j] - pts_3d_norm[b][i][j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>通过融合Gauss-Newton和梯度下降的思想，迭代更新<code>p</code>，直到收敛或达到最大迭代次数。</li></ul><p>然后，计算俩俩对应的3D点之间的欧氏距离，通过距离大小过滤匹配错误的点。</p><p>最后，使用密集描述子在小范围内遍历微调匹配位置，确保匹配的准确性。</p><p>至此，得到每个3D点在源图像中的投影位置<code>p</code>，然后通过<code>idx_i2j</code>将这些投影位置映射到源图像的像素索引。</p><h1 id="后端优化"><a href="#后端优化" class="headerlink" title="后端优化"></a>后端优化</h1><p>后端优化采用因子图建立拓扑关系，对于每一关键帧，找到当前关键帧上一帧关键帧，以及与当前关键帧最相似的历史关键帧，通过这些关键帧之间的相对位姿关系，构建因子图。</p><p>后端优化的核心是<code>solve_GN_calib()</code>函数，使用Gauss-Newton方法最小化重投影误差来优化关键帧位姿。该函数实现了MASt3R-SLAM论文中的BA部分以及滑动窗口优化策略，确保最早的几个关键帧保持固定，只优化最近的关键帧，避免过度约束。</p><p>同样，后端优化也使用CUDA实现，在<code>gauss_newton_calib</code>函数中，输入为：</p><ul><li><code>pose_data</code>，使用Sim3表示的位姿参数</li><li><code>Xs</code>，3D点云</li><li><code>Cs</code>，置信度</li><li><code>K</code>，相机内参</li><li><code>ii, jj</code>, 边索引</li><li><code>idx_ii2jj</code>, 点匹配索引</li><li><code>valid_match</code>, 有效匹配掩码</li><li><code>Q_ii2jj</code>, 匹配质量</li><li><code>height, width</code>, 图像尺寸</li><li><code>pixel_border</code>, 像素边界</li><li><code>z_eps</code>, 深度阈值</li><li><code>sigma_pixel</code>, 像素噪声标准差</li><li><code>sigma_depth</code>, 深度噪声标准差</li><li><code>C_thresh</code>, 置信度阈值</li><li><code>Q_thresh</code>, 质量阈值</li><li><code>max_iter</code>, 最大迭代次数</li><li><code>delta_thresh</code>, 收敛阈值</li></ul><p>采用Guass-Newton方法，首先计算每个点的重投影误差，然后构建雅可比矩阵和Hessian矩阵，最后通过迭代更新位姿参数，直到收敛或达到最大迭代次数。基于CUDA的并行计算，实现大规模优化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前端跟踪&quot;&gt;&lt;a href=&quot;#前端跟踪&quot; class=&quot;headerlink&quot; title=&quot;前端跟踪&quot;&gt;&lt;/a&gt;前端跟踪&lt;/h1&gt;&lt;p&gt;MASt3R-SLAM的核心在于利用MASt3R模型进行稠密的多视图立体匹配，而不是传统SLAM中的稀疏特征点匹配。MASt</summary>
      
    
    
    
    <category term="三维重建" scheme="https://blog.fengyunji.site/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="MASt3R" scheme="https://blog.fengyunji.site/tags/MASt3R/"/>
    
    <category term="SLAM" scheme="https://blog.fengyunji.site/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>无人艇集群编队控制</title>
    <link href="https://blog.fengyunji.site/%E6%97%A0%E4%BA%BA%E8%89%87%E9%9B%86%E7%BE%A4%E7%BC%96%E9%98%9F%E6%8E%A7%E5%88%B6/"/>
    <id>https://blog.fengyunji.site/%E6%97%A0%E4%BA%BA%E8%89%87%E9%9B%86%E7%BE%A4%E7%BC%96%E9%98%9F%E6%8E%A7%E5%88%B6/</id>
    <published>2025-07-08T09:11:51.000Z</published>
    <updated>2025-07-08T12:44:51.533Z</updated>
    
    <content type="html"><![CDATA[<p>无人艇编队控制项目是国家自然基金委重大项目的一个子课题，大课题由上海大学牵头，在我入学之前，已经完成了理论部分的研究，我在项目中负责结题演示实验的设计和实施。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="无人艇集群.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    无人艇集群：“小黄鸭群”  </p></div><h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>演示总体分为四个部分，分别控制20艘无人艇展示：</p><ol><li>无人艇从初始状态切换编队</li><li>无人艇在编队状态下前进，根据当前数量自主调整编队</li><li>在遭遇未知舰艇时，排除侦察艇前出侦察</li><li>若侦察艇发现中立舰艇，则增派一艘舰艇持续跟踪驱离</li><li>若侦察艇发现敌方舰艇，则增派两艘舰艇进行围捕（形成三艇围捕队形10秒视为围捕成功）</li></ol><h2 id="具体工作"><a href="#具体工作" class="headerlink" title="具体工作"></a>具体工作</h2><p>这个项目分为两个时间节点，一个是2023年年底的结题报告，一个是2024年4月的专家验收。</p><p>和之前参与过的项目不同，无人艇搭载的通讯设备是电台，采用UDP传输信息，不能使用ROS的消息传递机制，因此需要自己设计通信协议。这部分虽然很简单，但在实际的实验中由于发送速度过快或过慢，都遇到了不少问题。</p><p>算法的设计很简单，根据无人艇的数量，预设一个内外两个圈的队形，核心是要护航的一艘有人驾驶的船作为母艇。在有外来船只被感知时，根据不同的目标类型，重新设置编队位置，并使用匈牙利算法在最小化每艘船的位置变化条件下，分配无人艇到新的位置。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="遥控器视角.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    无人艇遥控器视角  </p></div><p>由于无人艇行驶在海面中，航程范围极大，因此传统的ego-planner等以栅格地图为基础的路径规划方法运算量极大，无法做到实时规划。因此，我们改进了人工势场法，避开歧义值，并在扇形区域内有船只时动态调整航向和速度，保证稳定安全的路径规划。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="专家演示演练.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    结题演示演练  </p></div><p>与无人机中使用的px4ctrl类似，控制算法采用了PD控制器，由于无人艇在海面上会收到风浪的影响，因此控制算法输入的目标也是一个围绕目标点的动态点，保证无人艇在风浪影响下仍能稳定前进并保持队形。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="编队.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    编队队形  </p></div><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="驱离.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    驱离队形  </p></div><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="围捕.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    围捕队形  </p></div>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;无人艇编队控制项目是国家自然基金委重大项目的一个子课题，大课题由上海大学牵头，在我入学之前，已经完成了理论部分的研究，我在项目中负责结题演示实验的设计和实施。&lt;/p&gt;
&lt;div style=&quot;display: flex; flex-direction: column; al</summary>
      
    
    
    
    <category term="科创项目" scheme="https://blog.fengyunji.site/categories/%E7%A7%91%E5%88%9B%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="课题组纵向项目" scheme="https://blog.fengyunji.site/tags/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%BA%B5%E5%90%91%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>SmartSharkVI</title>
    <link href="https://blog.fengyunji.site/SmartSharkVI/"/>
    <id>https://blog.fengyunji.site/SmartSharkVI/</id>
    <published>2025-07-01T08:30:19.000Z</published>
    <updated>2025-07-08T08:52:39.994Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.fengyunji.site/SmartSharkIV/">《Smart Shark IV——加入车队》</a><br><a href="https://blog.fengyunji.site/SmartSharkV/">《Smart Shark V——主力队员》</a></p><p>2022赛季是我作为主力队员在无人方程式车队的最后一个赛季，在这个赛季中，我们主要围绕八字绕环赛项，在弯道稳定性和速度上进行提升。同时，采用百度的Fast Deploy方案部署了最快最新的PPYOLOE_Plus模型，提升了感知的速度。</p><h2 id="力矩分配控制"><a href="#力矩分配控制" class="headerlink" title="力矩分配控制"></a>力矩分配控制</h2><p>在八字绕环中，赛车需要无限逼近横向摩擦力的极限，才能在弯道中获得更高的速度。在22赛季中，机械组和电驱动组首次在Smart Shark中采用了四电机独立驱动方式，这也使得控制算法增加了三个控制变量。对此，我们研发了力矩分配算法，在保证总驱动力不变的前提下，动态调整四个电机的输出力矩，以达到MPC状态量中期望的横摆角速度，在同样的弯道中能获得更高的过弯速度。</p><p>同时，我们搭建了数值仿真软件，能够加载自定义的地图和多种车辆模型，包括四电机模型、自行车模型，以及油门-转向MPC和力矩分配MPC等控制方式。通过引入Bayes优化器，能够自动优化MPC的参数，提升车辆的稳定性和速度。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="simulator.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    RK4数值仿真器  </p></div><p>在实际测试中，能够达到<strong>5.5m&#x2F;s</strong>速度下的稳定绕环</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="八字绕环.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    八字绕环实车效果  </p></div><p>相关成果已整理成论文《MPC-QPCA: Nonlinear MPC and Torque Allocation  for Four-Motor Autonomous Race Cars》投递至《Vehicle System Dynamics》</p><p>在这个赛季结束后，我便不再参与车队的核心工作，在2023到现在的几年时间里，我仍在参与车队的系统设计和算法开发，但不再参与实车测试和调试工作。这一段车队的经历为我研究生阶段的深入学习打下了基础，在本科的几年里，投入了无数假期，一有时间就去车队，赛车作为一个我从小时候就开始的梦想，终于在大学阶段得以实现。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://blog.fengyunji.site/SmartSharkIV/&quot;&gt;《Smart Shark IV——加入车队》&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://blog.fengyunji.site/SmartSharkV/&quot;&gt;《Sma</summary>
      
    
    
    
    <category term="科创项目" scheme="https://blog.fengyunji.site/categories/%E7%A7%91%E5%88%9B%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="无人驾驶方程式" scheme="https://blog.fengyunji.site/tags/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%96%B9%E7%A8%8B%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Smart Shark V</title>
    <link href="https://blog.fengyunji.site/SmartSharkV/"/>
    <id>https://blog.fengyunji.site/SmartSharkV/</id>
    <published>2025-06-30T13:36:23.000Z</published>
    <updated>2025-07-01T09:01:21.210Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.fengyunji.site/SmartSharkIV/">《Smart Shark IV——加入车队》</a></p><h2 id="方程式大赛队长会"><a href="#方程式大赛队长会" class="headerlink" title="方程式大赛队长会"></a>方程式大赛队长会</h2><p>2020赛季结束后，承蒙学长们的信任，我开始担任无人系统组的组长，负责带领团队进行无人驾驶算法的研发和测试工作。</p><p>在寒假返校后，车队受邀作为无人方程式的代表队在队长会上分享我们的无人驾驶系统，而我作为分享人跟随队长前往上海蔚来汽车总部。同时参与了上海车展，见证了蔚来汽车发布ET7、发布第二代换电站。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="蔚来发布会.jpg" alt="蔚来汽车发布会" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    蔚来汽车发布会    </p></div><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="第二代换电站.jpg" alt="蔚来第二代换电站" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    蔚来第二代换电站  </p></div><p>结束了上海车展，我们前往蔚来总部举行队长会（应蔚来要求禁止拍摄），我将车队的无人系统分为感知、规划、决策、感知、仿真和实车测试六个部分进行介绍，分享了我们在无人驾驶方程式方面的经验和技术。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="队长会PPT.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    队长会无人系统分享  </p></div><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="队长会合影.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    队长会合影  </p></div><p>队长会结束后，我们前往同济大学嘉定校区参观，与同济大学的无人方程式车队交流，分享了我们在无人驾驶方面的经验和技术。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="同济方程式.jpg" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    同济方程式车队  </p></div><h2 id="车队工作"><a href="#车队工作" class="headerlink" title="车队工作"></a>车队工作</h2><p>在2021赛季中，我们重点优化了以下几方面：</p><h3 id="感知融合算法迭代"><a href="#感知融合算法迭代" class="headerlink" title="感知融合算法迭代"></a>感知融合算法迭代</h3><p>得益于百度PPYOLO系列算法的开发，我们也将2020赛季使用的YOLOv4算法迭代为PPYOLO算法，并通过百度的fastinference套件部署于实车，实现了更高的推理速度和精度。</p><h3 id="Graph-SLAM"><a href="#Graph-SLAM" class="headerlink" title="Graph SLAM"></a>Graph SLAM</h3><p>我们向奇石乐(Kistler)公司申请了光流速度传感器的赞助，能够以200Hz的频率获取高精度的速度信息，基于准确的速度估计，我们开发因子图优化的Graph SLAM算法，能够在卫星定位失效的时候保证定位准确。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="GraphSLAM.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    Graph SLAM实车定位效果  </p></div><h3 id="离线MPC策略"><a href="#离线MPC策略" class="headerlink" title="离线MPC策略"></a>离线MPC策略</h3><p>高速巡迹是整个比赛中最重要的项目，在赛前并不知道地图，可多次尝试，每次跑动跑三圈，得分取多次跑动中最快的一次。因此，我们可以先慢速跑一次，建立锥桶地图，然后在赛道外离线计算最优路径，以此为目标路径交给MPC控制器进行跟踪控制。</p><p>我们将离线轨迹优化问题抽象为最优控制问题：</p><div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'Courier New', monospace;">$$ \begin{aligned} \min_{\mathbf{x},\mathbf{u}} \quad &\int_{s_0}^{s_f} \frac{dt}{ds} ds \\\\ \text{s.t.} \quad &\tilde{x}(s) \in \mathcal{X}_{\text{DoubleTrack}} \\\\ &\text{s.t. } x \in \mathcal{X}_{\text{Static}} \\\\ &\tilde{x}(s) \in \mathcal{X}_{\text{Kinematic}} \\\\ &\tilde{x}(s_f) = \tilde{x}(s_0) \\\\ &x(s) \in \mathcal{X}_{\text{Boundary}} \end{aligned} $$</div><p>同时，在MPC控制器的参数设定方法上，引入Bayes优化方法，将整个跑动过程包装为一个黒盒模型，通过在往年比赛的赛道中不断优化，使得整体跑动耗时减少了**2.35%**。</p><h3 id="实际效果"><a href="#实际效果" class="headerlink" title="实际效果"></a>实际效果</h3><p>我们将上述迭代部署至实车中：</p><p>直线加速达到最快速度<strong>18m&#x2F;s</strong>，<strong>5.93s</strong>完成比赛</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="直线加速.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    直线加速  </p></div><p>高速巡迹赛项最快速度达到<strong>10.06m&#x2F;s</strong></p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="高速循迹.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    高速巡迹  </p></div><p>凭借优秀的算法设计，在大家的共同努力下，车队再次获得FSAC的<strong>冠军</strong>。</p><h2 id="车队GitHub开源2-0"><a href="#车队GitHub开源2-0" class="headerlink" title="车队GitHub开源2.0"></a>车队GitHub开源2.0</h2><p>为了促进各个大学车队的共同发展，我们将部分算法：MPC控制算法、相机和点云联合感知算法、仿真器、锥桶数据集进行了开源，并在GitHub上发布。</p><p>官微：<a href="https://www.bitfsd.com/archives/4437">开源 | 北京理工大学FSAC开源平台2.0上线</a><br>GitHub算法：<a href="https://github.com/bitfsd">BITFSD</a></p><p>开源结构包括：</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="开源2.0.png" alt="队长会无人系统分享" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    BITFSD开源2.0结构  </p></div><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=332269842&bvid=BV11A411N7w2&cid=314145797&p=1"           width="800"           height="450"           scrolling="no"           border="0"           frameborder="no"           framespacing="0"           allowfullscreen="true"          style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">  </iframe>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    BITFSD 开源2.0仿真跑动效果  </p></div>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://blog.fengyunji.site/SmartSharkIV/&quot;&gt;《Smart Shark IV——加入车队》&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;方程式大赛队长会&quot;&gt;&lt;a href=&quot;#方程式大赛队长会&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="科创项目" scheme="https://blog.fengyunji.site/categories/%E7%A7%91%E5%88%9B%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="无人驾驶方程式" scheme="https://blog.fengyunji.site/tags/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%96%B9%E7%A8%8B%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Smart Shark IV</title>
    <link href="https://blog.fengyunji.site/SmartSharkIV/"/>
    <id>https://blog.fengyunji.site/SmartSharkIV/</id>
    <published>2025-06-30T11:44:07.000Z</published>
    <updated>2025-07-01T09:01:21.189Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无人驾驶方程式车队"><a href="#无人驾驶方程式车队" class="headerlink" title="无人驾驶方程式车队"></a>无人驾驶方程式车队</h2><p>官网链接：<a href="https://www.bitfsd.com/">https://www.bitfsd.com/</a></p><p>北京理工大学无人驾驶方程式车队 (Beijing Institute of Technology Formula Student Driverless Team, BITFSD) 成立于2015年9月，车队工作室位于北京市房山区北京理工大学良乡校区工程训练中心，现有来自我校不同学院的本科生、研究生共计100余人，是一支以“大学生无人驾驶方程式大赛”等各类创新创业大赛为牵引、瞄向无人车辆高端技术前沿的科技创新团队。</p><p>车队每赛季自主设计研发一辆全新的无人驾驶方程式赛车——“灰鲨 (Smart Shark) ”系列，具备环境感知、定位导航、独立驱动及并联式线控底盘等关键技术，以“中国大学生无人驾驶方程式大赛”为主要竞赛项目，并灵活参加其他比赛及社会活动。</p><h2 id="加入BITFSD"><a href="#加入BITFSD" class="headerlink" title="加入BITFSD"></a>加入BITFSD</h2><p>我一直对科创有着极大的热情，在7月结束高考后就与自动化学院的RoboMaster机器人战队建立了联系，2019年考入北理工，我参与了机器人队的入队培训，同时也积极地参与到睿信书院（自动化、计算机、光电三大学院）开展的各类科创活动中。但在对机器人队逐渐的熟悉的过程里，我发现团队的氛围似乎并不太适合我，于是在完成培训后并没有参加入队考核，不过在此期间学习的SolidWorks软件对我硕士阶段设计无人机帮助极大。在2019年12月份左右的时候，中国大学生方程式大赛<a href="http://www.formulastudent.com.cn/">FSC</a>落幕，无人方程式车队开始招新，他们制作的招新视频展示了无人驾驶技术作为一项最前沿的科技，而我有机会能够参与其中，这对于刚刚进入大学的我来说具有极大的诱惑力，于是我果断报名了车队。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=79440740&bvid=BV1KJ41167X7&cid=135942081&p=1"           width="800"           height="450"           scrolling="no"           border="0"           frameborder="no"           framespacing="0"           allowfullscreen="true"          style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">  </iframe>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    BITFSD 2019招新宣传视频  </p></div><p>其实一开始我的志愿是去机械组，因为我认为机械才是最真实的浪漫，但好在无人组的学长看重了我高中阶段参加信息学竞赛的经历，于是我顺理成章加入了无人系统组，在此特别感谢泰然学长！</p><h2 id="参与工作"><a href="#参与工作" class="headerlink" title="参与工作"></a>参与工作</h2><p>进入车队参与正式工作后，恰逢疫情原因完全在做仿真的算法，能够在仿真里控制方程式赛车完成无人驾驶，让当时刚大一的我获得了巨大的成就感，不过这仅仅是开胃菜。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="2020仿真.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    仿真算法测试  </p></div><p>我和学长们一起重构了无人系统的控制框架，采用动力学-运动学融合模型，使用ACADO求解器实现了 <strong>50步200Hz</strong> 的控制求解速度，并同步重写了各个赛项的规划策略。</p><p>以第n作者身份在中国汽车工程学会主办的年会技术论坛中发表了论文《Real-Time Motion Planning and Control for a Formula Student》</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="2020控制论文.png" alt="2020年会技术论坛" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    2020年会技术论坛论文  </p></div><p>等到2020年底回到学校后，我第一次参与了实车的测试，这是我第一次见到算法在方程式赛车上控制无人驾驶</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <video width="800" height="450" controls style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">    <source src="2020试车跑动.mp4" type="video/mp4">    您的浏览器不支持视频标签。  </video>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    Smart Shark IV 2020试车跑动  </p></div><p>算法从仿真到实车有一个非常大的gap，有许多硬件的原因需要调整，我们花费了一整个国庆假期，在卡丁车俱乐部租场地调试</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="2020年试车.jpg" alt="2020试车" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    2020年试车现场  </p></div><h2 id="比赛"><a href="#比赛" class="headerlink" title="比赛"></a>比赛</h2><p>由于疫情影响，我没有作为正式队员参与线下赛事，在FSAC 2020年的比赛中，Smart Shark IV获得了<strong>冠军</strong>，并打破了“直线加速”赛项的中国记录，最高时速达到<strong>10m&#x2F;s</strong>，能够勉强看到ETH的AMZ车队尾灯。</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=759211060&bvid=BV1864y1z7r3&cid=371655929&p=1"           width="800"           height="450"           scrolling="no"           border="0"           frameborder="no"           framespacing="0"           allowfullscreen="true"          style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">  </iframe>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    FSAC 2020赛季Smart Shark IV直线加速比赛  </p></div><p>同时，高速巡迹赛项也获得了冠军。（车载go pro拍摄）</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=246545979&bvid=BV14v411e764&cid=294361355&p=1"           width="800"           height="450"           scrolling="no"           border="0"           frameborder="no"           framespacing="0"           allowfullscreen="true"          style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">  </iframe>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    FSAC 2020赛季Smart Shark IV高速巡迹比赛  </p></div><p>在这一年里，我还跟着车队前往央视拍摄了2020中国汽车风云盛典</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="2020中国汽车风云盛典.jpg" alt="2020中国汽车风云盛典" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    2020中国汽车风云盛典  </p></div><p>见到了CBD那栋需要用广角镜头才能拍下来的大楼</p><div style="display: flex; flex-direction: column; align-items: center; margin: 20px 0;">  <img src="2020CBD大楼.jpg" alt="CBD大楼" loading="lazy" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>  <p style="margin-top: 8px; margin-bottom: 0; font-size: 14px; color: #666; font-style: italic; text-align: center;">    CBD大楼广角拍摄  </p></div><p>这一年是我学习科创的第一年。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;无人驾驶方程式车队&quot;&gt;&lt;a href=&quot;#无人驾驶方程式车队&quot; class=&quot;headerlink&quot; title=&quot;无人驾驶方程式车队&quot;&gt;&lt;/a&gt;无人驾驶方程式车队&lt;/h2&gt;&lt;p&gt;官网链接：&lt;a href=&quot;https://www.bitfsd.com/&quot;&gt;htt</summary>
      
    
    
    
    <category term="科创项目" scheme="https://blog.fengyunji.site/categories/%E7%A7%91%E5%88%9B%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="无人驾驶方程式" scheme="https://blog.fengyunji.site/tags/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%96%B9%E7%A8%8B%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Zotero添加坚果云存储</title>
    <link href="https://blog.fengyunji.site/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    <id>https://blog.fengyunji.site/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/</id>
    <published>2025-06-16T13:07:32.000Z</published>
    <updated>2025-07-01T09:01:21.321Z</updated>
    
    <content type="html"><![CDATA[<h2 id="设置文件同步"><a href="#设置文件同步" class="headerlink" title="设置文件同步"></a>设置文件同步</h2><ol><li>登录坚果云网站：<a href="https://www.jianguoyun.com/">https://www.jianguoyun.com/</a></li><li>点击账户信息<br><img src="/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/1.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></li><li>选中安全选项<br><img src="/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/2.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></li><li>在Zotero的同步中选择<code>WebDAV</code>，将安全选项中右侧的服务器地址、账号和密码复制到Zotero的同步中<br><img src="/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/3.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></li><li>点击<code>验证服务器</code>，如果成功则会显示<code>文件同步设定成功</code>，可返回正常同步数据</li></ol><h2 id="设置百度API翻译"><a href="#设置百度API翻译" class="headerlink" title="设置百度API翻译"></a>设置百度API翻译</h2><ol><li>下载Translate for Zotero插件：<a href="https://zotero-chinese.com/plugins/">https://zotero-chinese.com/plugins/</a></li><li>在设置中选择<code>百度垂直领域</code><br><img src="/Zotero%E6%B7%BB%E5%8A%A0%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AD%98%E5%82%A8/4.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></li><li>前往<a href="https://fanyi-api.baidu.com/manage/developer%E8%8E%B7%E5%8F%96%E5%BA%94%E7%94%A8ID%E5%92%8CSecret">https://fanyi-api.baidu.com/manage/developer获取应用ID和Secret</a> Key</li></ol><h2 id="设置GPT"><a href="#设置GPT" class="headerlink" title="设置GPT"></a>设置GPT</h2><ol><li>下载插件：Awesome GPT</li><li>进入无问芯穹：<a href="https://cloud.infini-ai.com/genstudio/model">https://cloud.infini-ai.com/genstudio/model</a></li><li>可自主选择模型，以DeepSeek-R1为例，复制API Key</li><li>在Zotero的设置中选择<code>Awesome GPT</code>，Base API填写”<a href="https://cloud.infini-ai.com/maas%22%EF%BC%8CAPI">https://cloud.infini-ai.com/maas&quot;，API</a> Key填写刚才复制的API Key，Model选择’deepseek-r1’</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;设置文件同步&quot;&gt;&lt;a href=&quot;#设置文件同步&quot; class=&quot;headerlink&quot; title=&quot;设置文件同步&quot;&gt;&lt;/a&gt;设置文件同步&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;登录坚果云网站：&lt;a href=&quot;https://www.jianguoyun.com/&quot;&gt;ht</summary>
      
    
    
    
    <category term="博客" scheme="https://blog.fengyunji.site/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="Zotero" scheme="https://blog.fengyunji.site/tags/Zotero/"/>
    
    <category term="坚果云" scheme="https://blog.fengyunji.site/tags/%E5%9D%9A%E6%9E%9C%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>宇树Go2机器狗高层控制命令仿真</title>
    <link href="https://blog.fengyunji.site/%E5%AE%87%E6%A0%91Go2%E6%9C%BA%E5%99%A8%E7%8B%97%E9%AB%98%E5%B1%82%E6%8E%A7%E5%88%B6%E5%91%BD%E4%BB%A4%E4%BB%BF%E7%9C%9F/"/>
    <id>https://blog.fengyunji.site/%E5%AE%87%E6%A0%91Go2%E6%9C%BA%E5%99%A8%E7%8B%97%E9%AB%98%E5%B1%82%E6%8E%A7%E5%88%B6%E5%91%BD%E4%BB%A4%E4%BB%BF%E7%9C%9F/</id>
    <published>2025-04-28T13:35:08.000Z</published>
    <updated>2025-07-01T09:01:21.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="宇树Go2机器狗"><a href="#宇树Go2机器狗" class="headerlink" title="宇树Go2机器狗"></a>宇树Go2机器狗</h2><p>宇树(Unitree)Go2是一款高性能的四足机器人，是宇树科技推出的高级消费级和教育用机器狗产品，官方的仿真不支持高层控制命令，比如速度控制、指点控制等，仅支持关节控制。这里介绍如何在Gazebo中实现高层控制命令仿真。</p><h2 id="仿真环境"><a href="#仿真环境" class="headerlink" title="仿真环境"></a>仿真环境</h2><ul><li>Ubuntu 20.04</li><li>ROS Noetic</li></ul><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>项目基于legged_control仓库搭建，引入宇树Go2机器狗的URDF模型，使用Gazebo仿真，使用MPC控制算法控制机器狗的运动。</p><ol><li><p>下载所有仓库，可用<a href="https://gh-proxy.com/github.com/%E6%9B%BF%E6%8D%A2https://github.com/">https://gh-proxy.com/github.com/替换https://github.com/</a></p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p unitree_ws/src</span><br><span class="line"><span class="built_in">cd</span> unitree_ws/src</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Feng1909/legged_control_go2.git</span><br><span class="line"><span class="comment"># 克隆OCS2</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/leggedrobotics/ocs2.git</span><br><span class="line"><span class="comment"># 克隆pinocchio</span></span><br><span class="line">git <span class="built_in">clone</span> --recurse-submodules https://github.com/leggedrobotics/pinocchio.git</span><br><span class="line"><span class="comment"># 克隆hpp-fcl</span></span><br><span class="line">git <span class="built_in">clone</span> --recurse-submodules https://github.com/leggedrobotics/hpp-fcl.git</span><br><span class="line"><span class="comment"># 克隆ocs2_robotic_assets</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/leggedrobotics/ocs2_robotic_assets.git</span><br><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line">sudo apt install liburdfdom-dev liboctomap-dev libassimp-dev</span><br></pre></td></tr></table></figure></li><li><p>编译工作空间</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">catkin config -DCMAKE_BUILD_TYPE=RelWithDebInfo</span><br><span class="line">catkin build ocs2_legged_robot_ros ocs2_self_collision_visualization</span><br><span class="line">catkin build legged_controllers legged_unitree_description</span><br></pre></td></tr></table></figure></li><li><p>运行仿真</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定机器狗模型</span></span><br><span class="line"><span class="built_in">export</span> ROBOT_TYPE=go2</span><br><span class="line"><span class="comment"># 加载Gazebo模型</span></span><br><span class="line">roslaunch legged_unitree_description empty_world.launch</span><br></pre></td></tr></table></figure><p> 此时可以看到Gazebo中加载了Go2机器狗模型，但是没有控制命令，机器狗不会动。接下来需要加载控制命令。</p> <img src="sim.png" alt="go2_sim" style="zoom:50%;"   data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'/></li><li><p>启动控制器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roslaunch legged_controllers load_controller.launch cheater:=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p> 终端可以输入期望的步态</p> <img src="controller.png" alt="controller" style="zoom:50%;"   data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'/> 可选的步态有：   * stance * trot * standing_trot * flying_trot * pace * standing_pace * dynamic_walk * static_walk * amble * lindyhop * skipping * pawup 先输入`stance`，新起一个终端，有两种方法可以启动控制器 1. 命令行启动     <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rosservice call /controller_manager/switch_controller <span class="string">&quot;start_controllers: [&#x27;controllers/legged_controller&#x27;]                   </span></span><br><span class="line"><span class="string">stop_controllers: [&#x27;&#x27;]</span></span><br><span class="line"><span class="string">strictness: 0</span></span><br><span class="line"><span class="string">start_asap: false</span></span><br><span class="line"><span class="string">timeout: 0.0&quot;</span></span><br></pre></td></tr></table></figure> 2. 可视化界面启动     <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-noetic-rqt-controller-manager</span><br><span class="line">rosrun rqt_controller_manager rqt_controller_manager</span><br></pre></td></tr></table></figure>     选择`/controller_manager`，右键点击`controllers/legged_controller`，点击`Start`按钮，启动控制器。     <img src="controller_manager.png" alt="controller_manager" style="zoom:50%;"   data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'/> 3. 发布控制指令     可使用/cmd_vel指定速度控制，但是需要将`stance`修改为移动状态的一种  <p> 当然，也可以使用<code>rqt</code>命令加载插件的方式将几个终端放在一起，需要<code>import</code>提前在<code>legged_control_go2</code>文件夹下设置好的<code>Default.perspective</code>文件</p><p> <video src="unitree_control.mp4" controls="controls" width="100%" height="100%"></video></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;宇树Go2机器狗&quot;&gt;&lt;a href=&quot;#宇树Go2机器狗&quot; class=&quot;headerlink&quot; title=&quot;宇树Go2机器狗&quot;&gt;&lt;/a&gt;宇树Go2机器狗&lt;/h2&gt;&lt;p&gt;宇树(Unitree)Go2是一款高性能的四足机器人，是宇树科技推出的高级消费级和教育用机器</summary>
      
    
    
    
    <category term="具身智能" scheme="https://blog.fengyunji.site/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="仿真" scheme="https://blog.fengyunji.site/tags/%E4%BB%BF%E7%9C%9F/"/>
    
  </entry>
  
  <entry>
    <title>KLT光流跟踪算法</title>
    <link href="https://blog.fengyunji.site/KLT%E5%85%89%E6%B5%81%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95/"/>
    <id>https://blog.fengyunji.site/KLT%E5%85%89%E6%B5%81%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95/</id>
    <published>2025-04-19T03:21:12.000Z</published>
    <updated>2025-07-01T09:01:21.188Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>光流的概念是Gibson在1950年首先提出来的。<br>它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。一般而言，光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。</p><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ol><li>相邻帧间亮度恒定</li><li>相邻帧间运动微小</li><li>相邻帧间像素点具有强相关性（空间一致性）</li></ol><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>由1：$I(x,y,t)&#x3D;I(x+dx,y+dy,t+dt)$</p><p>由2：$I(x,y,t)&#x3D;I(x,y,t)+\frac{\partial I}{\partial x}\Delta x+\frac{\partial I}{\partial y}\Delta y+\frac{\partial I}{\partial t}\Delta t+\epsilon$</p><p>其中$\epsilon$是高阶无穷小，可以忽略不计<br>推导出：$\frac{\partial I}{\partial x}\Delta x+\frac{\partial I}{\partial y}\Delta y+\frac{\partial I}{\partial t}\Delta t&#x3D;0$</p><p>两边同除 $dt$：$\frac{\partial I}{\partial x}\frac{\Delta x}{\Delta t}+\frac{\partial I}{\partial y}\frac{\Delta y}{\Delta t}+\frac{\partial I}{\partial t}&#x3D;0$</p><p>设$u, v$为速度矢量，$I_x, I_y$为灰度的方向梯度，$I_t$为灰度的时间梯度，可得：$I_xu+I_yv+I_t&#x3D;0$</p><p>超定方程，无法求解。</p><p>由3：在相邻像素点间存在强相关性，可得线性方程组</p><div>$$\left [    \begin{matrix}    I_x(p_1) & I_y(p_1)\\    ... & ... \\    I_x(p_N) & I_y(p_N)\end{matrix}\right ]\left [    \begin{matrix}    u \\ v    \end{matrix}\right ] = -\left [    \begin{matrix}    I_t(p_1) \\ ... \\ I_t(p_N)    \end{matrix}\right ]$$</div><p>即：<br>$$A \cdot d &#x3D; b$$<br>$$(A^TA)d&#x3D;A^Tb$$<br>可得到Hessian矩阵$A^TA$，使用最小二乘法求解$d$，即为光流矢量。</p><p>注：Hessian矩阵的特征值需要同时较大，否则最小二乘会遇到病态矩阵，此时取角点能够满足要求。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;光流的概念是Gibson在1950年首先提出来的。&lt;br&gt;它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>训练DUSt3R模型</title>
    <link href="https://blog.fengyunji.site/%E8%AE%AD%E7%BB%83DUSt3R%E6%A8%A1%E5%9E%8B/"/>
    <id>https://blog.fengyunji.site/%E8%AE%AD%E7%BB%83DUSt3R%E6%A8%A1%E5%9E%8B/</id>
    <published>2025-03-08T03:28:03.000Z</published>
    <updated>2025-07-01T09:01:21.417Z</updated>
    
    <content type="html"><![CDATA[<p>DUSt3R的介绍可参考本博客<a href="https://blog.fengyunji.site/Dust3r/">DUSt3R模型》</a></p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p>根据<a href="https://github.com/naver/dust3r">DUSt3R的github</a>的说明，安装依赖</p><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>数据集分为很多个，需要分别下载</p><h3 id="CO3D数据集"><a href="#CO3D数据集" class="headerlink" title="CO3D数据集"></a>CO3D数据集</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p data/co3d_subset</span><br><span class="line"><span class="built_in">cd</span> data/co3d_subset</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/facebookresearch/co3d</span><br><span class="line"><span class="built_in">cd</span> co3d</span><br><span class="line">python3 ./co3d/download_dataset.py --download_folder ../ --single_sequence_subset</span><br><span class="line"><span class="built_in">rm</span> ../*.zip</span><br><span class="line"><span class="built_in">cd</span> ../../..</span><br><span class="line"></span><br><span class="line">python3 datasets_preprocess/preprocess_co3d.py --co3d_dir data/co3d_subset --output_dir data/co3d_subset_processed  --single_sequence_subset</span><br></pre></td></tr></table></figure><h3 id="ARKitScenes数据集"><a href="#ARKitScenes数据集" class="headerlink" title="ARKitScenes数据集"></a>ARKitScenes数据集</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> data/</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/apple/ARKitScenes.git</span><br><span class="line">python3 download_data.py raw --video_id_csv raw/raw_train_val_splits.csv --download_dir arkit_data/ --raw_dataset_assets lowres_depth vga_wide vga_wide_intrinsics lowres_wide.traj</span><br></pre></td></tr></table></figure><p>主要需要提前安装<code>zip</code>和<code>unzip</code>，否则会报错</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;DUSt3R的介绍可参考本博客&lt;a href=&quot;https://blog.fengyunji.site/Dust3r/&quot;&gt;DUSt3R模型》&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装依赖&quot;&gt;&lt;a href=&quot;#安装依赖&quot; class=&quot;headerlink&quot; title=&quot;安装</summary>
      
    
    
    
    <category term="环境搭建" scheme="https://blog.fengyunji.site/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="DUSt3R" scheme="https://blog.fengyunji.site/tags/DUSt3R/"/>
    
    <category term="MASt3R" scheme="https://blog.fengyunji.site/tags/MASt3R/"/>
    
    <category term="具身智能" scheme="https://blog.fengyunji.site/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>habitat-sim和habitat-lab环境搭建</title>
    <link href="https://blog.fengyunji.site/habitat-sim%E5%92%8Chabitat-lab%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>https://blog.fengyunji.site/habitat-sim%E5%92%8Chabitat-lab%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2025-02-25T13:43:36.000Z</published>
    <updated>2025-07-01T09:01:21.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>承接上文搭建斯坦福大学GibsonEnv环境失败，在本文中重新基于habitat-sim开始搭建环境。</p><h2 id="habitat-sim环境搭建"><a href="#habitat-sim环境搭建" class="headerlink" title="habitat-sim环境搭建"></a>habitat-sim环境搭建</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n habitat python=3.9 cmake=3.14.0</span><br><span class="line">conda activate habitat</span><br><span class="line">conda install habitat-sim withbullet -c conda-forge -c aihabitat</span><br></pre></td></tr></table></figure><h2 id="habitat-lab环境搭建"><a href="#habitat-lab环境搭建" class="headerlink" title="habitat-lab环境搭建"></a>habitat-lab环境搭建</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --branch stable https://github.com/facebookresearch/habitat-lab.git</span><br><span class="line"><span class="built_in">cd</span> habitat-lab</span><br><span class="line">pip install -e habitat-lab  <span class="comment"># install habitat_lab</span></span><br><span class="line">pip install -e habitat-baselines  <span class="comment"># install habitat_baselines</span></span><br></pre></td></tr></table></figure><p>在完成环境安装后，可以使用<code>python examples/example.py</code>来测试环境是否安装成功。一定会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Requested RearrangeDataset config paths <span class="string">&#x27;data/datasets/replica_cad/rearrange/v2/train/rearrange_easy.json.gz&#x27;</span> or <span class="string">&#x27;data/replica_cad/&#x27;</span> are not downloaded locally. Aborting.</span><br></pre></td></tr></table></figure><p>这需要我们下载数据集，下载的是replicaCAD数据集，但是下载方式需要特别注意，需要参考这个地方<a href="https://github.com/facebookresearch/habitat-sim/blob/dfb388e29e5e1f25da4b576305e85bdc0be140b8/src_python/habitat_sim/utils/datasets_download.py#L341">链接</a>去分开下载数据集，否则数据集不全，需要执行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m habitat_sim.utils.datasets_download --uids &lt;replica_datasets&gt; --data-path data/</span><br></pre></td></tr></table></figure><p><code>replica_dataset</code>为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;replica_cad_dataset&quot;</span>,</span><br><span class="line"><span class="string">&quot;hab_fetch&quot;</span>,</span><br><span class="line"><span class="string">&quot;ycb&quot;</span>,</span><br><span class="line"><span class="string">&quot;rearrange_pick_dataset_v0&quot;</span>,</span><br><span class="line"><span class="string">&quot;rearrange_dataset_v2&quot;</span>,</span><br></pre></td></tr></table></figure><p>但是，在下载的时候还有坑。直接运行命令在下载完解压的时候会出现：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: unknown shorthand flag: <span class="string">&#x27;f&#x27;</span> <span class="keyword">in</span> -f</span><br></pre></td></tr></table></figure><p>这是关于git-lfs的报错，可能与当时的版本有关系，因此完整命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m habitat_sim.utils.datasets_download --uids &lt;replica_datasets&gt; --data-path data/ --no-prune</span><br></pre></td></tr></table></figure><p>安装完成后，可以使用<code>python examples/example.py</code>来测试环境是否安装成功。</p><h2 id="相机渲染测试"><a href="#相机渲染测试" class="headerlink" title="相机渲染测试"></a>相机渲染测试</h2><p>可运行以下代码，通过<code>WSAD</code>控制相机移动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> habitat</span><br><span class="line"><span class="keyword">from</span> habitat.sims.habitat_simulator.actions <span class="keyword">import</span> HabitatSimActions</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FORWARD_KEY=<span class="string">&quot;w&quot;</span></span><br><span class="line">LEFT_KEY=<span class="string">&quot;a&quot;</span></span><br><span class="line">RIGHT_KEY=<span class="string">&quot;d&quot;</span></span><br><span class="line">FINISH=<span class="string">&quot;f&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_rgb_bgr</span>(<span class="params">image</span>):</span><br><span class="line">    <span class="keyword">return</span> image[:, :, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>():</span><br><span class="line">    env = habitat.Env(</span><br><span class="line">        config=habitat.get_config(<span class="string">&quot;benchmark/nav/pointnav/pointnav_habitat_test.yaml&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Environment creation successful&quot;</span>)</span><br><span class="line">    observations = env.reset()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Destination, distance: &#123;:3f&#125;, theta(radians): &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        observations[<span class="string">&quot;pointgoal_with_gps_compass&quot;</span>][<span class="number">0</span>],</span><br><span class="line">        observations[<span class="string">&quot;pointgoal_with_gps_compass&quot;</span>][<span class="number">1</span>]))</span><br><span class="line">    cv2.imshow(<span class="string">&quot;RGB&quot;</span>, transform_rgb_bgr(observations[<span class="string">&quot;rgb&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Agent stepping around inside environment.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    count_steps = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> env.episode_over:</span><br><span class="line">        keystroke = cv2.waitKey(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> keystroke == <span class="built_in">ord</span>(FORWARD_KEY):</span><br><span class="line">            action = HabitatSimActions.move_forward</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;action: FORWARD&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> keystroke == <span class="built_in">ord</span>(LEFT_KEY):</span><br><span class="line">            action = HabitatSimActions.turn_left</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;action: LEFT&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> keystroke == <span class="built_in">ord</span>(RIGHT_KEY):</span><br><span class="line">            action = HabitatSimActions.turn_right</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;action: RIGHT&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> keystroke == <span class="built_in">ord</span>(FINISH):</span><br><span class="line">            action = HabitatSimActions.stop</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;action: FINISH&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;INVALID KEY&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        observations = env.step(action)</span><br><span class="line">        count_steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Destination, distance: &#123;:3f&#125;, theta(radians): &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            observations[<span class="string">&quot;pointgoal_with_gps_compass&quot;</span>][<span class="number">0</span>],</span><br><span class="line">            observations[<span class="string">&quot;pointgoal_with_gps_compass&quot;</span>][<span class="number">1</span>]))</span><br><span class="line">        cv2.imshow(<span class="string">&quot;RGB&quot;</span>, transform_rgb_bgr(observations[<span class="string">&quot;rgb&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Episode finished after &#123;&#125; steps.&quot;</span>.<span class="built_in">format</span>(count_steps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (</span><br><span class="line">        action == HabitatSimActions.stop</span><br><span class="line">        <span class="keyword">and</span> observations[<span class="string">&quot;pointgoal_with_gps_compass&quot;</span>][<span class="number">0</span>] &lt; <span class="number">0.2</span></span><br><span class="line">    ):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;you successfully navigated to destination point&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;your navigation was unsuccessful&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    example()</span><br></pre></td></tr></table></figure><p>gibson数据集下载:<br><img src="gibson.jpeg"   data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'/></p><!-- <img src="/habitat-sim和habitat-lab环境搭建/gibson.jpeg"   data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'/> -->]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;p&gt;承接上文搭建斯坦福大学GibsonEnv环境失败，在本文中重新基于habitat-sim开始搭建环境。&lt;/p&gt;
&lt;h2 id=&quot;habita</summary>
      
    
    
    
    <category term="环境搭建" scheme="https://blog.fengyunji.site/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="具身智能" scheme="https://blog.fengyunji.site/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>斯坦福大学GibsonEnv环境搭建</title>
    <link href="https://blog.fengyunji.site/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6GibsonEnv%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>https://blog.fengyunji.site/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6GibsonEnv%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2025-02-25T08:16:17.000Z</published>
    <updated>2025-07-01T09:01:21.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>为了在3DGS三维重建的时候获得更加真实的场景数据，摒弃Gazebo自建场景，这些场景只方便测试控制算法，对于感知数据的真实性难以保证。下面尝试Gibson环境，已尝试Matterport3D搭建失败。  </p><p>前置环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt-get install libglew-dev libglm-dev libassimp-dev xorg-dev libglu1-mesa-dev libboost-dev \</span><br><span class="line">mesa-common-dev freeglut3-dev libopenmpi-dev cmake golang libjpeg-turbo8-dev wmctrl \</span><br><span class="line">xdotool libzmq3-dev zlib1g-dev</span><br></pre></td></tr></table></figure><h2 id="尝试python3-8"><a href="#尝试python3-8" class="headerlink" title="尝试python3.8"></a>尝试python3.8</h2><p>使用python3.8创建环境，为了后期适配ROS。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gibsonenv python=3.8 anaconda</span><br><span class="line">conda activate gibsonenv</span><br></pre></td></tr></table></figure><p>为了对应CUDA 11.8，采用torch 2.0.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure><p>安装tensorflow-gpu</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-gpu==2.2.0</span><br></pre></td></tr></table></figure><p>克隆仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/StanfordVL/GibsonEnv.git</span><br><span class="line"><span class="built_in">cd</span> GibsonEnv</span><br><span class="line">./download.sh </span><br><span class="line">./build.sh build_local</span><br></pre></td></tr></table></figure><p>首先会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">GibsonEnv/gibson/core/channels/external/glew.c:2774:1: error: unknown <span class="built_in">type</span> name ‘PFNGLCLEARDEPTHFOESPROC’; did you mean ‘PFNGLCLEARDEPTHFPROC’?</span><br><span class="line"> 2774 | PFNGLCLEARDEPTHFOESPROC __glewClearDepthfOES = NULL;</span><br><span class="line">      | ^~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">      | PFNGLCLEARDEPTHFPROC</span><br><span class="line">/home/adipandas/git_clones/GibsonEnv/gibson/core/channels/external/glew.c:2774:48: warning: initialization of ‘int’ from ‘void *’ makes <span class="built_in">integer</span> from pointer without a cast [-Wint-conversion]</span><br><span class="line"> 2774 | PFNGLCLEARDEPTHFOESPROC __glewClearDepthfOES = NULL;</span><br><span class="line">      |                                                ^~~~</span><br><span class="line">/home/adipandas/git_clones/GibsonEnv/gibson/core/channels/external/glew.c:2775:1: error: unknown <span class="built_in">type</span> name ‘PFNGLCLIPPLANEFOESPROC’; did you mean ‘PFNGLCLIPPLANEFPROC’?</span><br><span class="line"> 2775 | PFNGLCLIPPLANEFOESPROC __glewClipPlanefOES = NULL;</span><br><span class="line">      | ^~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">      | PFNGLCLIPPLANEFPROC</span><br><span class="line">/home/adipandas/git_clones/GibsonEnv/gibson/core/channels/external/glew.c:2775:46: warning: initialization of ‘int’ from ‘void *’ makes <span class="built_in">integer</span> from pointer without a cast [-Wint-conversion]</span><br><span class="line"> 2775 | PFNGLCLIPPLANEFOESPROC __glewClipPlanefOES = NULL;</span><br><span class="line">      |                                              ^~~~</span><br><span class="line">/home/adipandas/git_clones/GibsonEnv/gibson/core/channels/external/glew.c:2776:1: error: unknown <span class="built_in">type</span> name ‘PFNGLDEPTHRANGEFOESPROC’; did you mean ‘PFNGLDEPTHRANGEFPROC’?</span><br><span class="line"> 2776 | PFNGLDEPTHRANGEFOESPROC __glewDepthRangefOES = NULL;</span><br><span class="line">      | ^~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">      | PFNGLDEPTHRANGEFPROC</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>需要在<code>GibsonEnv/gibson/core/channels/external/glew.c</code>中把没有的全换成对应的，应将2774-2779行修改为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PFNGLCLEARDEPTHFPROC __glewClearDepthfOES = <span class="literal">NULL</span>;</span><br><span class="line">PFNGLCLIPPLANEFPROC __glewClipPlanefOES = <span class="literal">NULL</span>;</span><br><span class="line">PFNGLDEPTHRANGEFPROC __glewDepthRangefOES = <span class="literal">NULL</span>;</span><br><span class="line">PFNGLFRUSTUMFPROC __glewFrustumfOES = <span class="literal">NULL</span>;</span><br><span class="line">PFNGLGETCLIPPLANEFPROC __glewGetClipPlanefOES = <span class="literal">NULL</span>;</span><br><span class="line">PFNGLORTHOFPROC __glewOrthofOES = <span class="literal">NULL</span>;</span><br></pre></td></tr></table></figure><p>再次编译即可成功，随后安装python包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p>首先报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lib/python3.10/site-packages/setuptools/_distutils/dist.py:268: UserWarning: Unknown distribution option: <span class="string">&#x27;tests_require&#x27;</span></span><br></pre></td></tr></table></figure><p>需要在<code>setup.py</code>中把<code>tests_require</code>改为:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#     tests_require=[],</span></span><br><span class="line">extras_require=&#123;</span><br><span class="line">        <span class="string">&#x27;test&#x27;</span>: [<span class="string">&#x27;pytest&#x27;</span>],</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>再次报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: Multiple top-level packages discovered <span class="keyword">in</span> a flat-layout:</span><br></pre></td></tr></table></figure><p>在setup.py中添加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">setup(name=<span class="string">&#x27;gibson&#x27;</span>,</span><br><span class="line">    version=<span class="string">&#x27;0.3.1&#x27;</span>,</span><br><span class="line">    py_modules=[],</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>好的，现在让我们运行<code>python examples/demo/play_husky_nonviz.py</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Segmentation fault (core dumped)</span><br></pre></td></tr></table></figure><p>至此，在Ubuntu 20.04上提高python、pytorch、tensorflow、CUDA版本安装的尝试宣告失败</p><h2 id="docker运行"><a href="#docker运行" class="headerlink" title="docker运行"></a>docker运行</h2><p>能用，但感觉不太好用</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;为了在3DGS三维重建的时候获得更加真实的场景数据，摒弃Gazebo自建场景，这些场景只方便测试控制算法，对于感知数据的真实性难以保证。下面</summary>
      
    
    
    
    <category term="环境搭建" scheme="https://blog.fengyunji.site/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="具身智能" scheme="https://blog.fengyunji.site/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>git submodule使用</title>
    <link href="https://blog.fengyunji.site/submodule/"/>
    <id>https://blog.fengyunji.site/submodule/</id>
    <published>2024-12-23T08:02:27.000Z</published>
    <updated>2025-07-01T09:01:21.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="在项目中使用子模块"><a href="#在项目中使用子模块" class="headerlink" title="在项目中使用子模块"></a>在项目中使用子模块</h2><h3 id="添加子模块"><a href="#添加子模块" class="headerlink" title="添加子模块"></a>添加子模块</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule add +仓库地址 +本地路径</span><br></pre></td></tr></table></figure><p>比如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule add https://github.com/xxx/xxx.git ./submodule/xxx</span><br></pre></td></tr></table></figure><p>如果提示该仓库已经存在，应清除该仓库的缓存：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">rm</span> --cached ./submodule/xxx</span><br></pre></td></tr></table></figure><h3 id="克隆项目"><a href="#克隆项目" class="headerlink" title="克隆项目"></a>克隆项目</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recurse-submodules +仓库地址</span><br></pre></td></tr></table></figure><h3 id="更新子模块"><a href="#更新子模块" class="headerlink" title="更新子模块"></a>更新子模块</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><p>其中<code>--init</code>表示初始化子模块，<code>--recursive</code>表示递归更新子模块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;在项目中使用子模块&quot;&gt;&lt;a href=&quot;#在项目中使用子模块&quot; class=&quot;headerlink&quot; title=&quot;在项目中使用子模块&quot;&gt;&lt;/a&gt;在项目中使用子模块&lt;/h2&gt;&lt;h3 id=&quot;添加子模块&quot;&gt;&lt;a href=&quot;#添加子模块&quot; class=&quot;header</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://blog.fengyunji.site/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>对极几何</title>
    <link href="https://blog.fengyunji.site/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95/"/>
    <id>https://blog.fengyunji.site/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95/</id>
    <published>2024-11-08T06:53:44.000Z</published>
    <updated>2025-07-01T09:01:21.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DUSt3R"><a href="#DUSt3R" class="headerlink" title="DUSt3R"></a>DUSt3R</h2><p>DUSt3R在描述相对位姿估计都时候提到</p><blockquote><p>One way is to perform 2D matching and recover intrinsics as described above, then estimate the Epipolar matrix and recover the relative pose</p></blockquote><p>相对位姿估计的一种方法是执行2D匹配并恢复如上所述的内部函数，然后估计极线矩阵并恢复相对姿态，另一种方法是直接比较点地图。当然，DUSt3R采用的是后者。</p><h2 id="对极几何"><a href="#对极几何" class="headerlink" title="对极几何"></a>对极几何</h2><p>考虑如下场景：<br><img src="/对极几何/对极几何.png"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><br>主要思想是：</p><ol><li>给定两张影像，获得对应的3D坐标点对。</li><li>先计算基础矩阵F，</li><li>然后结合相机内参矩阵K，恢复本质矩阵E。</li><li>对E进行SVD分解，得到R和t。</li></ol><h3 id="对极约束"><a href="#对极约束" class="headerlink" title="对极约束"></a>对极约束</h3><p>假设世界坐标系下点坐标为$P$，像素坐标系下点坐标为$p$，相机内参矩阵为$K$，相机外参矩阵为$R, t$，则有：<br>$$dp&#x3D;KP$$<br>其中$d$为齐次化矩阵，对于两个相机，有：<br>$$d_0p_0&#x3D;KP$$<br>$$d_1p_1&#x3D;KP$$<br>已知$R, t$，则有：<br>$$d_1p_1&#x3D;K(RP+t)$$<br>令$x&#x3D;K^{-1}p$，则有：<br>$$d_0x_0&#x3D;P$$<br>$$d_1x_1&#x3D;RP+t$$<br>即：<br>$$d_1x_1&#x3D;R(d_0x_0)+t$$<br>两边同时叉乘$t$，有：<br>$$t\times d_1x_1&#x3D;t\times R(d_0x_0)$$<br>$$t\times x_1&#x3D;t\times Rx_0$$<br>两边同时左乘$x^T_1$，有：<br>$$x^T_1t\times x_1&#x3D;x^T_1t\times Rx_0$$<br>得到：<br>$$0&#x3D;x^T_1t\times R^Tx_0$$</p><h3 id="本征矩阵"><a href="#本征矩阵" class="headerlink" title="本征矩阵"></a>本征矩阵</h3><p>由对极约束，本征矩阵$E$满足：<br>$$E&#x3D;t\times R$$<br>$$x^T_1Ex_0&#x3D;0$$<br>多组点可以求得$E$，然后对$E$进行SVD分解，得到$R, t$。</p><h3 id="基础矩阵"><a href="#基础矩阵" class="headerlink" title="基础矩阵"></a>基础矩阵</h3><p>基础矩阵$F$满足：<br>$$F&#x3D;K^{-T}EK^{-1}$$<br>是在本征矩阵基础上加入了相机内参矩阵的约束。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol><li>通过对极几何可以恢复相机的相对姿态，但是无法恢复绝对姿态。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;DUSt3R&quot;&gt;&lt;a href=&quot;#DUSt3R&quot; class=&quot;headerlink&quot; title=&quot;DUSt3R&quot;&gt;&lt;/a&gt;DUSt3R&lt;/h2&gt;&lt;p&gt;DUSt3R在描述相对位姿估计都时候提到&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One way is to</summary>
      
    
    
    
    <category term="三维重建" scheme="https://blog.fengyunji.site/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="SFM" scheme="https://blog.fengyunji.site/tags/SFM/"/>
    
  </entry>
  
  <entry>
    <title>orin nx 重装</title>
    <link href="https://blog.fengyunji.site/orin-nx-%E9%87%8D%E8%A3%85/"/>
    <id>https://blog.fengyunji.site/orin-nx-%E9%87%8D%E8%A3%85/</id>
    <published>2024-11-04T08:38:30.000Z</published>
    <updated>2025-07-01T09:01:21.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="刷写系统镜像"><a href="#刷写系统镜像" class="headerlink" title="刷写系统镜像"></a>刷写系统镜像</h2><ol><li>使用USB线缆连接ORIN的OTG-USB端口与Ubuntu系统开发主机的USB端口</li><li>将REC按键按下不松开，然后给系统供电，按键保持3秒以上，释放REC按键</li><li>在主机终端显示界面输入 lsusb，若有显示NVIDIA设备，则成功进入Recovery恢复模式，此时可进行后续刷机，备份等操作</li><li>下载<a href="https://developer.nvidia.com/sdk-manager">Orin NX SDK</a></li><li>刷写Jetson 5.1.3 （注：只能刷5.1.3，5.1.2和5.1.4都不行）<br> <img src="/orin-nx-%E9%87%8D%E8%A3%85/1.png" alt="刷写镜像"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><br> 刷写完成后点击skip，然后点击finish</li><li>重启设备，进入系统</li><li>安装阿木的补丁 <a href="https://download.amovlab.com/orin_nx/amov-orin-jetson3550.deb">https://download.amovlab.com/orin_nx/amov-orin-jetson3550.deb</a><br><strong>注：</strong> Jetson Pack 5.x是Ubuntu 20.04，Jetson Pack 6.x是Ubuntu 22.04</li></ol><h2 id="安装GPU相关库"><a href="#安装GPU相关库" class="headerlink" title="安装GPU相关库"></a>安装GPU相关库</h2><ol><li>安装CUDA 11.4<br><code>sudo apt-get install cuda-11.4</code></li><li>安装cuDNN 8<br> <code>sudo apt-get install libcudnn8</code></li><li>安装TensorRT<br> 下载这个tar包和deb包<br> <img src="/orin-nx-%E9%87%8D%E8%A3%85/2.png" alt="TensorRT"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><ol><li>系统安装TensorRT <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i nv-tensorrt-repo-ubuntu2004-cuda11.4-trt8.2.5.1-ga-20220505_1-1_arm64.deb</span><br><span class="line">sudo apt-get install tensorrt</span><br></pre></td></tr></table></figure></li><li>安装TensorRT的Python库<br> 解压刚刚下载的tar包，进入python文件夹，pip安装对应版本的whl文件</li></ol></li><li>OpenCV4安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://download.amovlab.com/model/install/common/opencv470-install.sh</span><br><span class="line"><span class="built_in">chmod</span> +x opencv470-install.sh</span><br><span class="line">./opencv470-install.sh</span><br></pre></td></tr></table></figure> 可以将sh脚本里的with_CUDA改为ON，以此安装VINS-GPU</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;刷写系统镜像&quot;&gt;&lt;a href=&quot;#刷写系统镜像&quot; class=&quot;headerlink&quot; title=&quot;刷写系统镜像&quot;&gt;&lt;/a&gt;刷写系统镜像&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;使用USB线缆连接ORIN的OTG-USB端口与Ubuntu系统开发主机的USB端口&lt;/li&gt;
</summary>
      
    
    
    
    <category term="无人机" scheme="https://blog.fengyunji.site/categories/%E6%97%A0%E4%BA%BA%E6%9C%BA/"/>
    
    
    <category term="Orin NX" scheme="https://blog.fengyunji.site/tags/Orin-NX/"/>
    
  </entry>
  
  <entry>
    <title>vins回环检测</title>
    <link href="https://blog.fengyunji.site/vins%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B/"/>
    <id>https://blog.fengyunji.site/vins%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B/</id>
    <published>2024-10-21T08:53:14.000Z</published>
    <updated>2025-07-01T09:01:21.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>在使用Mast3r进行三维重建的时候，原始方法是对n张图片通过n*(n-1)的复杂度进行遍历匹配，但是这种方法太费时：50张图需要2450次匹配，单次匹配需要0.3秒(Nvidia RTX 3060)，共计需要12分钟。</p><p>当手动选取具有共视区的图像对时，只选取了241组配对图片，只需要1.2分钟即可匹配完成，因此根据共视区选取图像对的方法，可以大大减少匹配时间。</p><h2 id="回环检测"><a href="#回环检测" class="headerlink" title="回环检测"></a>回环检测</h2><p>在当前的研究中，可以完成共视区计算的算法主要应用与SLAM的回环检测中，回环检测是SLAM中的一个重要环节，它可以提高地图的一致性，减少累积误差，提高定位的精度。常用算法有ORB、VINS等。</p><p>因此可以选用VINS的回环检测算法，通过计算共视区，选取具有共视区的图像对，减少匹配时间。</p><h2 id="VINS回环检测"><a href="#VINS回环检测" class="headerlink" title="VINS回环检测"></a>VINS回环检测</h2><p>VINS回环检测的主要流程如下：</p><ol><li>利用DBoW2进行回环检测。</li><li>除了用于单目VIO的角点特征外，还添加了500个角点并使用BRIEF描述子描述。额外的角点特征用于在回环检测中实现更好的召回率。</li><li>DBoW2在时间和空间一致性检查后返回回环检测候选帧。</li><li>VINS保留所有用于特征检索的BRIEF描述子，丢弃原始图像以减少内存消耗。</li><li>检测到回环时，通过BRIEF描述子匹配找到对应关系，建立局部滑动窗口与回环候选帧之间的连接。</li></ol><p>算法流程图为：<br><img src="/vins%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B/vins.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p><h3 id="DBoW2"><a href="#DBoW2" class="headerlink" title="DBoW2"></a>DBoW2</h3><p>DBoW2是一个用于图像检索的库，它可以用于回环检测。</p><p>代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">KeyFrame::computeWindowBRIEFPoint</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">BriefExtractor <span class="title">extractor</span><span class="params">(BRIEF_PATTERN_FILE.c_str())</span></span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; (<span class="type">int</span>)point_2d_uv.<span class="built_in">size</span>(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    cv::KeyPoint key;</span><br><span class="line">    key.pt = point_2d_uv[i];</span><br><span class="line">    window_keypoints.<span class="built_in">push_back</span>(key);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">extractor</span>(image, window_keypoints, window_brief_descriptors);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">KeyFrame::computeBRIEFPoint</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">BriefExtractor <span class="title">extractor</span><span class="params">(BRIEF_PATTERN_FILE.c_str())</span></span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> fast_th = <span class="number">20</span>; <span class="comment">// corner detector response threshold</span></span><br><span class="line"><span class="keyword">if</span>(<span class="number">1</span>)</span><br><span class="line">cv::<span class="built_in">FAST</span>(image, keypoints, fast_th, <span class="literal">true</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">vector&lt;cv::Point2f&gt; tmp_pts;</span><br><span class="line">cv::<span class="built_in">goodFeaturesToTrack</span>(image, tmp_pts, <span class="number">500</span>, <span class="number">0.01</span>, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; (<span class="type">int</span>)tmp_pts.<span class="built_in">size</span>(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    cv::KeyPoint key;</span><br><span class="line">    key.pt = tmp_pts[i];</span><br><span class="line">    keypoints.<span class="built_in">push_back</span>(key);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">extractor</span>(image, keypoints, brief_descriptors);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; (<span class="type">int</span>)keypoints.<span class="built_in">size</span>(); i++)</span><br><span class="line">&#123;</span><br><span class="line">Eigen::Vector3d tmp_p;</span><br><span class="line">m_camera-&gt;<span class="built_in">liftProjective</span>(Eigen::<span class="built_in">Vector2d</span>(keypoints[i].pt.x, keypoints[i].pt.y), tmp_p);</span><br><span class="line">cv::KeyPoint tmp_norm;</span><br><span class="line">tmp_norm.pt = cv::<span class="built_in">Point2f</span>(tmp_p.<span class="built_in">x</span>()/tmp_p.<span class="built_in">z</span>(), tmp_p.<span class="built_in">y</span>()/tmp_p.<span class="built_in">z</span>());</span><br><span class="line">keypoints_norm.<span class="built_in">push_back</span>(tmp_norm);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;研究背景&quot;&gt;&lt;a href=&quot;#研究背景&quot; class=&quot;headerlink&quot; title=&quot;研究背景&quot;&gt;&lt;/a&gt;研究背景&lt;/h2&gt;&lt;p&gt;在使用Mast3r进行三维重建的时候，原始方法是对n张图片通过n*(n-1)的复杂度进行遍历匹配，但是这种方法太费时：50张</summary>
      
    
    
    
    <category term="SLAM" scheme="https://blog.fengyunji.site/categories/SLAM/"/>
    
    
    <category term="vins" scheme="https://blog.fengyunji.site/tags/vins/"/>
    
    <category term="共视区" scheme="https://blog.fengyunji.site/tags/%E5%85%B1%E8%A7%86%E5%8C%BA/"/>
    
    <category term="特征点" scheme="https://blog.fengyunji.site/tags/%E7%89%B9%E5%BE%81%E7%82%B9/"/>
    
    <category term="回环检测" scheme="https://blog.fengyunji.site/tags/%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>nerfstudio</title>
    <link href="https://blog.fengyunji.site/nerfstudio/"/>
    <id>https://blog.fengyunji.site/nerfstudio/</id>
    <published>2024-09-20T06:38:01.000Z</published>
    <updated>2025-07-01T09:01:21.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="nerfstudio"><a href="#nerfstudio" class="headerlink" title="nerfstudio"></a>nerfstudio</h2><p>Nerfstudio，一个用于NeRF开发的模块化PyTorch框架。框架中用于实现基于NeRF的方法的组件即插即用，使得研究人员和相关从业者可以轻松地将NeRF集成到自己的项目中。框架的模块化设计支持实时可视化工具，导入用户真实世界捕获的数据集外（in-the-wild）数据，以及导出为视频，点云和网格表示的工具。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>官网安装方式可能会导致tiny-cuda-nn安装失败</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">conda create --name nerfstudio -y python=3.8</span><br><span class="line">conda activate nerfstudio</span><br><span class="line"></span><br><span class="line">conda install pytorch==2.1.2 torchvision==0.16.2 pytorch-cuda=11.8 -c pytorch -c nvidia</span><br><span class="line">pip install ninja git+https://gitee.com/Xiaoshier2021/tiny-cuda-nn/<span class="comment">#subdirectory=bindings/torch</span></span><br><span class="line">conda install -c <span class="string">&quot;nvidia/label/cuda-11.8.0&quot;</span> cuda-toolkit</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/nerfstudio-project/nerfstudio.git</span><br><span class="line"><span class="built_in">cd</span> nerfstudio</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h3 id="前处理数据"><a href="#前处理数据" class="headerlink" title="前处理数据"></a>前处理数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ns-process-data images --data ~/data_nerf/images/ --output-dir ~/data_nerf/out/</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ns-train nerfacto --data ~/data_nerf/out/ --vis viewer+tensorboard</span><br></pre></td></tr></table></figure><p>可视化工具：<a href="http://localhost:7007/">http://localhost:7007/</a></p><h3 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ns-eval --load-config outputs/out/nerfacto/2024-09-20_100756/config.yml --output_path outputs/out/nerfacto/result.json --render-output-path outputs/out/nerfacto/render</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;nerfstudio&quot;&gt;&lt;a href=&quot;#nerfstudio&quot; class=&quot;headerlink&quot; title=&quot;nerfstudio&quot;&gt;&lt;/a&gt;nerfstudio&lt;/h2&gt;&lt;p&gt;Nerfstudio，一个用于NeRF开发的模块化PyTorch框架。框架中</summary>
      
    
    
    
    <category term="Nerf" scheme="https://blog.fengyunji.site/categories/Nerf/"/>
    
    
    <category term="3D Reconstruction" scheme="https://blog.fengyunji.site/tags/3D-Reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>InstantSplat</title>
    <link href="https://blog.fengyunji.site/InstantSplat/"/>
    <id>https://blog.fengyunji.site/InstantSplat/</id>
    <published>2024-08-28T12:53:51.000Z</published>
    <updated>2025-07-01T09:01:21.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="InstantSplat"><a href="#InstantSplat" class="headerlink" title="InstantSplat"></a>InstantSplat</h1><p>github连接：<a href="https://github.com/NVlabs/InstantSplat">InstantSplat</a><br><img src="/InstantSplat/1.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><br>InstantSplat，这是一种高效的新视图合成（NVS）方法，专门设计用于处理大规模场景、稀疏视图和无姿态条件。InstantSplat通过结合3D高斯溅射（3D-GS）和端到端密集立体模型（DUSt3R）的优势，能够在不到一分钟内从稀疏和无姿态的图像中重建出高质量的3D场景，并生成准确的新视图。</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/InstantSplat/2.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p><ol><li><p>使用DUSt3R生成稠密点云</p></li><li><p>基于生成的稠密点云使用3D-GS进行优化，具体是进行1000次迭代，但不进行稠密化、分裂、重置透明度等操作</p></li><li><p>联合优化姿势和属性<br>根据公式</p><p>$$S*, T*&#x3D;{\arg \min}<em>{S, T} \sum</em>{v\in N}\sum^{HW}_{i&#x3D;1}||\widetilde{C}_v^i(S, T)-C_v^i(S, T)||+\lambda \cdot ||T-T0||$$</p><p>其中$S$代表3D高斯点集合，$T$代表相机外参，$C$代表渲染函数，$T0$代表初始值，上述公式保证了在优化位姿的时候不会过于远离初始值。</p></li><li><p>在测试视图上对齐相机姿态<br>在实际应用中，测试视图的相机姿态可能是未知的或带有噪声的，这会影响最终渲染图像的准确性和一致性。首先，在训练视图上训练了一个3DGS模型，并保持3DGS模型在测试阶段是冻结的，即不对其参数进行进一步的调整。然后，他们专注于单独优化测试视图的相机姿态，目标是最小化由这个模型渲染的图像与实际测试视图之间的光度差异。这个过程涉及到调整相机姿态，直到渲染出的图像与真实图像在视觉上尽可能相似。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;InstantSplat&quot;&gt;&lt;a href=&quot;#InstantSplat&quot; class=&quot;headerlink&quot; title=&quot;InstantSplat&quot;&gt;&lt;/a&gt;InstantSplat&lt;/h1&gt;&lt;p&gt;github连接：&lt;a href=&quot;https://gith</summary>
      
    
    
    
    <category term="三维重建" scheme="https://blog.fengyunji.site/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="GaussianSplating" scheme="https://blog.fengyunji.site/tags/GaussianSplating/"/>
    
    <category term="DUSt3R" scheme="https://blog.fengyunji.site/tags/DUSt3R/"/>
    
    <category term="InstantSplat" scheme="https://blog.fengyunji.site/tags/InstantSplat/"/>
    
  </entry>
  
  <entry>
    <title>DUSt3R</title>
    <link href="https://blog.fengyunji.site/Dust3r/"/>
    <id>https://blog.fengyunji.site/Dust3r/</id>
    <published>2024-08-28T00:55:06.000Z</published>
    <updated>2025-07-01T09:01:21.183Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DUST3r"><a href="#DUST3r" class="headerlink" title="DUST3r"></a>DUST3r</h2><p>github连接：<a href="https://github.com/naver/dust3r">DUSt3R</a><br><img src="/Dust3r/2.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'><br>DUSt3R是一种旨在简化几何3D视觉任务的新框架。作者着重于使3D重建过程更加易于使用和高效。该框架利用深度学习和几何处理的最新进展，提高了准确性并降低了计算复杂性。 </p><p>DUSt3R以一组不受约束的图像为输入，输出点图(pointmaps)，从这些点图可以直接推导出各种几何量。</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/Dust3r/1.png" alt="alt text"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><strong>输入：</strong> 两张不受约束的图像</p><p>通过共享权重的ViT网络生成两个token，返回在解码器中进行联合推理，在解码器传递过程中，两个分支之间不断共享信息。这对于输出正确对齐的点图至关重要。也就是说，每个解码器块关注来自另一个分支的token：<br>$$G_i^1 &#x3D; DecoderBlock_i^1 (G^1_{i-1}, G^2_{i-1})$$<br>$$G_i^2 &#x3D; DecoderBlock_i^2 (G^2_{i-1}, G^1_{i-1})$$<br><strong>输出：</strong> 经过Head处理后输出点云图和置信图，点云图的坐标系为第一张图片的坐标系。</p><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><ol><li><p>3D回归损失函数<br>$$\mathcal{l}_{regr}(v, i)&#x3D;||\frac{1}{z}X_i^{v,1}-\frac{1}{\overline{z}}\overline{X}_i^{v,1}||$$</p><p>$X^{n,m}$是指相机$n$得到的点云图$X^n$在相机$m$的坐标系下观测到的。上述公式用于计算像素$i\in D^v$在视图$v$中的3D回归损失。<br>$$\overline{z}&#x3D;norm(X^1,X^2) &#x3D; \frac{1}{|D^1|+|D^2|}\sum_{v\in{1,2}}\sum_{i\in D^v}||X_i^v||$$<br>代表平均距离，分别通过$z$和$\overline{z}$对预测和真实点图进行归一化，它们只是表示所有有效的点到原点的平均距离。</p></li><li><p>置信度感知损失<br>存在定义不明确的 3D 点，例如在天空中或半透明物体上。更一般地，图像中的某些部分通常比其他部分更难预测。因此，共同学习预测每个像素的分数，该分数表示网络对该特定像素的置信度。最终的训练目标是所有有效像素的置信度加权回归损失：<br>$$\mathcal{L}<em>{conf}&#x3D;\sum</em>{v\in{1,2}}\sum_{i\in D^v}\mathcal{C}<em>i^{v,1}\mathcal{l}</em>{regr}(v, i)-\alpha\log{C_i^{v,1}}$$</p></li></ol><h3 id="下游应用"><a href="#下游应用" class="headerlink" title="下游应用"></a>下游应用</h3><ol><li>点匹配<br>通过最近邻算法，建立两个图像像素之间的对应关系。</li><li>恢复相机内参<br>假设主点大致位于中心，像素为正方形，因此只需估计焦点$f$。</li><li>相对姿态估计<br>执行 2D 匹配并恢复内参矩阵，然后估计对极矩阵并恢复相对姿态；或者使用 Procrustes 对齐比较点图获得相对姿态<br>参考我的另一篇文章<a href="https://blog.fengyunji.site/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95/">对极几何</a></li><li>绝对姿态估计</li></ol><h3 id="全局对齐"><a href="#全局对齐" class="headerlink" title="全局对齐"></a>全局对齐</h3><ol><li>配对图<br>俩俩配对，构建一个连通图，根据两对的平均置信度测量它们的重叠，然后过滤掉低置信度的对。</li><li>全局优化</li><li>恢复相机参数</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;DUST3r&quot;&gt;&lt;a href=&quot;#DUST3r&quot; class=&quot;headerlink&quot; title=&quot;DUST3r&quot;&gt;&lt;/a&gt;DUST3r&lt;/h2&gt;&lt;p&gt;github连接：&lt;a href=&quot;https://github.com/naver/dust3r&quot;&gt;DUS</summary>
      
    
    
    
    <category term="三维重建" scheme="https://blog.fengyunji.site/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="GaussianSplating" scheme="https://blog.fengyunji.site/tags/GaussianSplating/"/>
    
    <category term="DUSt3R" scheme="https://blog.fengyunji.site/tags/DUSt3R/"/>
    
  </entry>
  
  <entry>
    <title>colmap处理流程</title>
    <link href="https://blog.fengyunji.site/colmap%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>https://blog.fengyunji.site/colmap%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</id>
    <published>2024-06-11T08:03:01.000Z</published>
    <updated>2025-07-01T09:01:21.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gaussian-Splatting中COLMAP处理流程"><a href="#Gaussian-Splatting中COLMAP处理流程" class="headerlink" title="Gaussian Splatting中COLMAP处理流程"></a>Gaussian Splatting中COLMAP处理流程</h2><ul><li>feature_extractor</li><li>exhaustive_matcher</li><li>mapper</li><li>image_undistorter</li></ul><h2 id="feature-extractor"><a href="#feature-extractor" class="headerlink" title="feature_extractor"></a>feature_extractor</h2><p>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">colmap feature_extractor \</span><br><span class="line">        --database_path /home/yugrp01/dataset//distorted/database.db \</span><br><span class="line">        --image_path /home/yugrp01/dataset//input \</span><br><span class="line">        --ImageReader.single_camera 1 \</span><br><span class="line">        --ImageReader.camera_model OPENCV \</span><br><span class="line">        --SiftExtraction.use_gpu 1</span><br></pre></td></tr></table></figure><p>作用：提取图像特征，保存在database.db中，默认提取SIFT特征</p><h2 id="exhaustive-matcher"><a href="#exhaustive-matcher" class="headerlink" title="exhaustive_matcher"></a>exhaustive_matcher</h2><p>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">colmap exhaustive_matcher \</span><br><span class="line">        --database_path /home/yugrp01/dataset//distorted/database.db \</span><br><span class="line">        --SiftMatching.use_gpu 1</span><br></pre></td></tr></table></figure><p>作用：建立SIFT匹配，仍保存在database.db中</p><h2 id="mapper"><a href="#mapper" class="headerlink" title="mapper"></a>mapper</h2><p>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">colmap mapper \</span><br><span class="line">        --database_path /home/yugrp01/dataset//distorted/database.db \</span><br><span class="line">        --image_path /home/yugrp01/dataset//input \</span><br><span class="line">        --output_path /home/yugrp01/dataset//distorted/sparse \</span><br><span class="line">        --Mapper.ba_global_function_tolerance=0.000001</span><br></pre></td></tr></table></figure><p>作用：相机位姿求解与优化，在sparse文件夹下生成多个文件夹，一个文件夹对应一个相机，可以使用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">colmap model_converter \</span><br><span class="line">        --input_path /home/yugrp01/dataset//distorted/sparse/0 \</span><br><span class="line">        --output_path /home/yugrp01/dataset//distorted/sparse \</span><br><span class="line">        --output_type TXT</span><br></pre></td></tr></table></figure><p>将<code>sparse/0</code>中的<code>.bin</code>文件转换为<code>.txt</code>文件<br>mapper生成的文件为：</p><ul><li><code>cameras.bin</code>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Camera list with one line of data per camera:</span><br><span class="line">#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]</span><br><span class="line"># Number of cameras: 1</span><br><span class="line">1 OPENCV 720 1280 1120.1152718874055 1173.5453696531381 360 640 0.056079336784625386 -0.085614936918791046 -0.0035479370208935067 -0.00036881030187927946</span><br></pre></td></tr></table></figure>  内容为：相机ID，相机模型(OPENCV)，图像宽度，图像高度，相机内参</li><li><code>images.bin</code><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Image list with two lines of data per image:</span><br><span class="line">#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME</span><br><span class="line">#   POINTS2D[] as (X, Y, POINT3D_ID)</span><br><span class="line"># Number of images: 151, mean observations per image: 899.41059602649011</span><br><span class="line">139 0.97558110892036964 0.024064873414301063 0.19083814762661425 0.10603387758494046 2.0978674295044142 1.9014249718018907 -4.6688458262732428 1 0141.jpg</span><br><span class="line">342.20053100585938 2.5576851367950439 -1 294.03756713867188 8.3915014266967773 -1 710.8546142578125 9.4258575439453125 -1 320.52203369140625 22.885553359985352 -1 320.52203369140625 22.885553359985352 -1 333.97128295898438 25.935688018798828 -1 333.97128295898438 25.935688018798828 -1 707.79693603515625 26.888103485107422 -1 406.88790893554688 29.064554214477539 -1 320.9920654296875 30.22673225402832 -1 320.9920654296875 30.22673225402832 -1 294.0234375 31.514423370361328 -1 324.99057006835938 38.868644714355469 -1 324.99057006835938 38.868644714355469 -1 331.5408935546875 62.630966186523438 -1 331.5408935546875 62.630966186523438 -1 292.57781982421875 69.072616577148438 -1 292.57781982421875 69.072616577148438 -1 302.60647583007812 71.223625183105469 -1 302.60647583007812 71.223625183105469 -1 317.67379760742188 74.406044006347656 -1 317.67379760742188 74.406044006347656 -1 286.7628173828125 74.998977661132812 -1 286.7628173828125 74.998977661132812 -1 329.06219482421875 76.793952941894531 -1 329.06219482421875 76.793952941894531 -1 312.201416015625 80.79327392578125 -1 312.201416015625 80.79327392578125 -1 309.02401733398438 ...</span><br></pre></td></tr></table></figure>  内容为：图像ID，相机位姿，相机ID，图像名称，图像特征点(包括3D点ID，大多数是-1)</li><li><code>points3D.bin</code>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 3D point list with one line of data per point:</span><br><span class="line">#   POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)</span><br><span class="line"># Number of points: 22731, mean track length: 5.9747041485196428</span><br><span class="line">22664 9.3397715374614645 -0.5320455884139641 -0.35230666689830648 174 173 169 0.54436590619680714 11 963 3 47</span><br></pre></td></tr></table></figure>  内容为：3D点ID，3D点坐标，3D点颜色，3D点误差，3D点在图像中的投影(图像ID，特征点ID)</li></ul><h2 id="image-undistorter"><a href="#image-undistorter" class="headerlink" title="image_undistorter"></a>image_undistorter</h2><p>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">colmap image_undistorter \</span><br><span class="line">        --image_path /home/yugrp01/dataset//input \</span><br><span class="line">        --input_path /home/yugrp01/dataset//distorted/sparse/0 \</span><br><span class="line">        --output_path /home/yugrp01/dataset/ \</span><br><span class="line">        --output_type COLMAP</span><br></pre></td></tr></table></figure><p>作用：去畸变，生成去畸变后的图像，保存在<code>/home/yugrp01/dataset/</code>中</p><h2 id="替代COLMAP"><a href="#替代COLMAP" class="headerlink" title="替代COLMAP"></a>替代COLMAP</h2><p>COLMAP在GS中的应用仅是通过特征点匹配来获得相机位姿，可以使用其他方法代替COLMAP，如ORB-SLAM2，VINS-Mono等<br>具体代码位于<code>scene/dataset_readers.py</code>的<code>readColmapSceneInfo</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cameras_extrinsic_file = os.path.join(path, <span class="string">&quot;sparse/0&quot;</span>, <span class="string">&quot;images.bin&quot;</span>)</span><br><span class="line">    cameras_intrinsic_file = os.path.join(path, <span class="string">&quot;sparse/0&quot;</span>, <span class="string">&quot;cameras.bin&quot;</span>)</span><br><span class="line">    cam_extrinsics = read_extrinsics_binary(cameras_extrinsic_file)</span><br><span class="line">    cam_intrinsics = read_intrinsics_binary(cameras_intrinsic_file)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    cameras_extrinsic_file = os.path.join(path, <span class="string">&quot;sparse/0&quot;</span>, <span class="string">&quot;images.txt&quot;</span>)</span><br><span class="line">    cameras_intrinsic_file = os.path.join(path, <span class="string">&quot;sparse/0&quot;</span>, <span class="string">&quot;cameras.txt&quot;</span>)</span><br><span class="line">    cam_extrinsics = read_extrinsics_text(cameras_extrinsic_file)</span><br><span class="line">    cam_intrinsics = read_intrinsics_text(cameras_intrinsic_file)</span><br><span class="line"></span><br><span class="line">reading_dir = <span class="string">&quot;images&quot;</span> <span class="keyword">if</span> images == <span class="literal">None</span> <span class="keyword">else</span> images</span><br><span class="line">cam_infos_unsorted = readColmapCameras(cam_extrinsics=cam_extrinsics, cam_intrinsics=cam_intrinsics, images_folder=os.path.join(path, reading_dir))</span><br><span class="line">cam_infos = <span class="built_in">sorted</span>(cam_infos_unsorted.copy(), key = <span class="keyword">lambda</span> x : x.image_name)</span><br></pre></td></tr></table></figure><p><code>cam_infos</code>中是每张图的位姿和图片参数，比如<code>cam_infos[:3]</code>为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[CameraInfo(uid=1, R=array([[ 0.38001315, -0.23005276,  0.89591614],</span><br><span class="line">[ 0.23318819,  0.96112063,  0.14788647],</span><br><span class="line">[-0.89510518,  0.15271826,  0.41888406]]), T=array([-4.40688372,  0.55051116, -2.53033356]), FovY=0.9846972407709619, FovX=0.6130229182895368, image=&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=709x1259 at 0x7F54D3CCA110&gt;, image_path=<span class="string">&#x27;/home/yugrp01/dataset/images/0001.jpg&#x27;</span>, image_name=<span class="string">&#x27;0001&#x27;</span>, width=709, height=1259), </span><br><span class="line">CameraInfo(uid=1, R=array([[ 0.39393992, -0.22690239,  0.89068886],</span><br><span class="line">[ 0.23024742,  0.96251379,  0.14336435],</span><br><span class="line">[-0.88983002,  0.14860187,  0.43141629]]), T=array([-4.41738619,  0.52536371, -2.47282881]), FovY=0.9846972407709619, FovX=0.6130229182895368, image=&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=709x1259 at 0x7F54D3CCA050&gt;, image_path=<span class="string">&#x27;/home/yugrp01/dataset/images/0002.jpg&#x27;</span>, image_name=<span class="string">&#x27;0002&#x27;</span>, width=709, height=1259), C</span><br><span class="line">ameraInfo(uid=1, R=array([[ 0.4211666 , -0.22125352,  0.87958261],</span><br><span class="line">[ 0.22593051,  0.96481198,  0.13451114],</span><br><span class="line">[-0.87839291,  0.14207294,  0.45633451]]), T=array([-4.45405079,  0.48200487, -2.35331303]), FovY=0.9846972407709619, FovX=0.6130229182895368, image=&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=709x1259 at 0x7F54D3CB3F50&gt;, image_path=<span class="string">&#x27;/home/yugrp01/dataset/images/0003.jpg&#x27;</span>, image_name=<span class="string">&#x27;0003&#x27;</span>, width=709, height=1259)]</span><br></pre></td></tr></table></figure><p>其中，<code>CameraInfo</code>包含了相机位姿，图片，图片路径等信息，<code>uid</code>为相机ID，<code>R</code>为旋转矩阵，<code>T</code>为平移向量，<code>FovY</code>和<code>FovX</code>为视场角，<code>image</code>为图片，<code>image_path</code>为图片路径，<code>image_name</code>为图片名称，<code>width</code>和<code>height</code>为图片宽高，仅需自行输入<code>R</code>，<code>T</code>即可</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Gaussian-Splatting中COLMAP处理流程&quot;&gt;&lt;a href=&quot;#Gaussian-Splatting中COLMAP处理流程&quot; class=&quot;headerlink&quot; title=&quot;Gaussian Splatting中COLMAP处理流程&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="三维重建" scheme="https://blog.fengyunji.site/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="GaussianSplating" scheme="https://blog.fengyunji.site/tags/GaussianSplating/"/>
    
    <category term="COLMAP" scheme="https://blog.fengyunji.site/tags/COLMAP/"/>
    
  </entry>
  
</feed>
